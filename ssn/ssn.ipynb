{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset initialize spent: 0.2926006317138672 ms\n",
      "training: 713, validation: 71\n",
      "Dataset initialize spent: 0.005573272705078125 ms\n",
      "training: 713, validation: 71\n",
      "training set size:  713\n",
      "testing set size:  71\n",
      "792\n",
      "../dataset/new_dataset/base/simulated_combine_female_short_outfits_genesis8_toulouse_formalmegawardrobepreset14_CDIG8Female_StandF/pitch_25_rot_0_shadow.npy\n",
      "../dataset/new_dataset/cache/mask/simulated_combine_female_short_outfits_genesis8_toulouse_formalmegawardrobepreset14_CDIG8Female_StandF/pitch_25_rot_0_mask.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ssn_dataset\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "\n",
    "# csv_file = \"/home/ysheng/Dataset/new_dataset/meta_data.csv\"\n",
    "# csv_file = \"~/Dataset/soft_shadow/train/metadata.csv\"\n",
    "# compose_transform = None\n",
    "ds_folder = '../dataset/new_dataset'\n",
    "training_dataset = ssn_dataset.SSN_Dataset(ds_folder, is_training = True)\n",
    "testing_dataset = ssn_dataset.SSN_Dataset(ds_folder, is_training = False)\n",
    "\n",
    "print('training set size: ', len(training_dataset))\n",
    "print('testing set size: ',len(testing_dataset))\n",
    "\n",
    "print(len(training_dataset.meta_data))\n",
    "print(training_dataset.meta_data[0][0])\n",
    "print(training_dataset.meta_data[0][1])\n",
    "\n",
    "# for j in range(10):\n",
    "#     for i in range(len(training_dataset)):\n",
    "#         data = training_dataset[i]\n",
    "# #         print(\"{} \\r\".format(i), flush=True, end=\"\")\n",
    "#         print(\"{} \".format(i))\n",
    "    \n",
    "# for i,data in enumerate(testing_dataset):\n",
    "#     print(\"{} \\r\".format(i), flush=True, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /home/ysheng/Documents/adobe_shadow_net/ssn/ssn_dataset.py(127)__getitem__()\n",
      "-> mask_img, shadow_bases = np.expand_dims(mask_img, axis=2), np.load(shadow_path)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask max:  tensor(1.)\n",
      "light max:  tensor(0.)\n",
      "shadow max:  tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(training_dataset)):\n",
    "    mask_img, light_img, shadow_img = training_dataset[i]\n",
    "    print('mask max: ', torch.max(mask_img))\n",
    "    print('light max: ', torch.max(light_img))\n",
    "    print('shadow max: ', torch.max(shadow_img))\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from utils.net_utils import show_batch, show_light_batch\n",
    "import numpy as np\n",
    "# from scipy.ndimage.filttraining_datasetort gaussian_filter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "dataloader = DataLoader(training_dataset, batch_size=1, shuffle=False, num_workers=48)\n",
    "valid_dataloader = DataLoader(testing_dataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total = 10\n",
    "begin = time.time()\n",
    "with tqdm(total=total) as t:\n",
    "    for i, (mask, light, shadow,) in enumerate(dataloader):\n",
    "        I_s, L_t, I_t = mask, light, shadow\n",
    "        t.update()\n",
    "        \n",
    "        if i >= total:\n",
    "            break\n",
    "        \n",
    "elapsed = time.time() - begin\n",
    "\n",
    "print('{} batch spent: {}s'.format(total, elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils\n",
    "\n",
    "def vis_light(light_tensor, new_ibl):\n",
    "    if not new_ibl:\n",
    "        batch, c, h, w = light_tensor.shape\n",
    "        tensor_ret = torch.zeros((batch * c, 1, h, w))\n",
    "        counter = 0\n",
    "\n",
    "        for i in range(batch):\n",
    "            for j in range(c):\n",
    "                tensor_ret[counter,:,:,:] = light_tensor[i,j,:,:]\n",
    "                counter += 1\n",
    "\n",
    "        return tensor_ret\n",
    "    else:\n",
    "        b,c,d = light_tensor.shape\n",
    "        ret = torch.zeros((b,16,32))\n",
    "        for bi in range(b):\n",
    "            cur_tensor = light_tensor[bi]\n",
    "            print('alpha: ',torch.min(cur_tensor[:,0]), torch.max(cur_tensor[:,0]))\n",
    "            print('beta: ',torch.min(cur_tensor[:,1]), torch.max(cur_tensor[:,1]))\n",
    "            print('intensity: ',torch.min(cur_tensor[:,2]), torch.max(cur_tensor[:,2]))\n",
    "            \n",
    "            for i in range(c):\n",
    "                alpha, beta, intensity = cur_tensor[i]\n",
    "                # alpha: [-pi, pi]\n",
    "                # beta: [-pi/2, pi/2]\n",
    "                x = (alpha + np.pi)/np.pi * 0.5\n",
    "                y = (beta + 0.5 * np.pi)/np.pi\n",
    "                ret[bi, int(y * 16), int(x * 32)] = intensity\n",
    "        \n",
    "        return ret\n",
    "                \n",
    "\n",
    "def print_min_max(batch, comment):\n",
    "    print('{} min: {}, max: {}'.format(comment, torch.min(batch), torch.max(batch)))\n",
    "\n",
    "def show_img_batch(batch_img):\n",
    "    grid = utils.make_grid(batch_img)\n",
    "    show_np = grid.detach().cpu().numpy().transpose((1,2,0))\n",
    "    print('show np size: ', show_np.shape)\n",
    "    print('show min: {}, max: {}'.format(np.min(show_np), np.max(show_np)))\n",
    "    plt.figure(figsize=(30,20))\n",
    "    plt.imshow(show_np)\n",
    "    plt.show()\n",
    "    \n",
    "for i, (mask, light, shadow) in enumerate(dataloader):\n",
    "    if i >=  3:\n",
    "        break\n",
    "        \n",
    "    # concatenate human mask and shadow mask\n",
    "    I_s, L_t, I_t = mask, light, shadow\n",
    "    print_min_max(I_s, 'mask')\n",
    "    print_min_max(L_t, 'light')\n",
    "    print_min_max(I_t, 'shadow')\n",
    "    # print(L_t.shape)\n",
    "    # L_t = vis_light(L_t, True)\n",
    "    # print('after vis',L_t.shape)\n",
    "    show_batch(I_s, 'img_src.png')\n",
    "    show_batch(L_t/torch.max(L_t), 'light.png')\n",
    "    \n",
    "#     I_t = I_t/torch.max(I_t)\n",
    "    print('shadow min: {}, max: {}'.format(torch.min(I_t), torch.max(I_t)))\n",
    "    b,c,h,w = I_t.shape\n",
    "    # I_t = torch.clamp(I_t/30.0, 0.0, 1.0)\n",
    "    for batch in range(b):\n",
    "        # I_t[batch, :, :, :] = I_t[batch, :, :, :]/torch.max(L_t[batch])\n",
    "        # I_t[batch, :, :, :] = I_t[batch, :, :, :]/(0.5 * torch.max(I_t[batch, :, :, :])\n",
    "        I_t[batch, :, :, :] = I_t[batch, :, :, :]/torch.sum(L_t[batch, :, :, :]) * 3.5\n",
    "        I_t[batch, :, :, :] = torch.sqrt(I_t[batch, :, :, :])\n",
    "        \n",
    "        I_t[batch, :, :, :] = torch.clamp(I_t[batch, :, :, :], 0.0, 1.0)\n",
    "        print('cur batch range: {}, {}'.format(torch.min(I_t[batch, :, :, :]), torch.max(I_t[batch, :, :, :])))\n",
    "        print('cur batch illuminace sum: {}'.format(torch.sum(L_t[batch, :, :, :])))\n",
    "    show_img_batch(I_t)\n",
    "    \n",
    "#     human_gt = I_t[:,0,:,:]\n",
    "#     shadow_gt = I_t[:,1,:,:]\n",
    "    \n",
    "#     batch_size, c, h, w = I_t.size()\n",
    "#     human_mask, shadow_mask = torch.zeros(batch_size, h, w, dtype= torch.float32), torch.zeros(batch_size, h, w, dtype= torch.float32)\n",
    "#     human_mask[torch.where(human_gt !=0)] = 1\n",
    "#     shadow_mask[torch.where(shadow_gt !=0)] = 1\n",
    "    \n",
    "# #     print(human_mask.size())\n",
    "# #     print(shadow_mask.size())\n",
    "   \n",
    "#     show_batch(human_mask.view(batch_size, 1, h, w))\n",
    "#     show_batch(shadow_mask.view(batch_size, 1, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn \n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "\n",
    "def to_numpy_img(t):\n",
    "    t_np = t.numpy()\n",
    "    t_np = np.transpose(t_np, (1,2,0))\n",
    "    return t_np\n",
    "\n",
    "test_tensor = torch.zeros(9,256, 256)\n",
    "test = to_numpy_img(test_tensor)\n",
    "test[128, 128, 0] = 1\n",
    "\n",
    "test_ = gaussian_filter(test, sigma = 10)\n",
    "seaborn.heatmap(test[:,:,0])\n",
    "seaborn.heatmap(test_[:,:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# todo, vectorize this process\n",
    "test_torch = torch.zeros(16,32)\n",
    "for h in range(test_torch.size()[0]):\n",
    "    test_torch[h,:] = abs(math.sin(h / 16.0 * 3.1415926)) + 0.001\n",
    "\n",
    "test_np = test_torch.detach().cpu().numpy()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(test_np,cmap='gray')light_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ssn_submodule import Up\n",
    "import torch\n",
    "\n",
    "x1 = torch.randn(1,512, 16,16)\n",
    "x2 = torch.randn(1,256, 32,32)\n",
    "\n",
    "model = Up(512,256)\n",
    "out = model(x1,x2)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/home/ysheng/Dataset/soft_shadow/single_human/notsimulated_combine_male_short_outfits_genesis8_armani_casualoutfit03_Base_Pose_Standing_A/ground_truth.txt'\n",
    "\n",
    "with open(data_file,'r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "lines = [c.strip() for c in content]\n",
    "data_num = int(len(lines)/6)\n",
    "\n",
    "def get_pos(pos_str):\n",
    "    x = float(pos_str[1])\n",
    "    y = float(pos_str[2])\n",
    "    z = float(pos_str[3])\n",
    "    return np.array([x,y,z])\n",
    "\n",
    "def get_spherical(pos):\n",
    "    x,y,z = pos\n",
    "    alpha = np.arctan2(z,x)\n",
    "    beta = np.arctan2(y, np.sqrt(x**2 + z**2))\n",
    "    return alpha, beta\n",
    "\n",
    "prefix_list, alpha_list, beta_list = [], [], []\n",
    "for i in range(data_num):\n",
    "    prefix_str = lines[6 * i + 0]\n",
    "    human_pos_str = lines[6 * i + 3].split()\n",
    "    light_pos_str = lines[6 * i + 4].split()\n",
    "    human_pos = get_pos(human_pos_str)\n",
    "    light_pos = get_pos(light_pos_str)\n",
    "    \n",
    "    light_dir = light_pos - human_pos\n",
    "    light_dir_norm = light_dir/np.linalg.norm(light_dir, 2)\n",
    "    alpha, beta = get_spherical(light_dir_norm)\n",
    "    # print('alpha: {}, beta: {} \\n'.format(alpha, beta))\n",
    "    alpha_list.append(alpha + np.pi)\n",
    "    beta_list.append(beta + np.pi)    \n",
    "    prefix_list.append(prefix_str)\n",
    "    \n",
    "alpha_ary = np.array(alpha_list)\n",
    "beta_ary = np.array(beta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def compute_IBL(alpha, beta):\n",
    "    h, w = 256, 256\n",
    "    IBL = np.zeros((h,w))\n",
    "    \n",
    "    two_pi = 2.0 * np.pi\n",
    "    alpha_coord, beta_coord = int(w * alpha/two_pi), int(h * beta/two_pi)\n",
    "    IBL[beta_coord, alpha_coord] = 1.0\n",
    "    return IBL\n",
    "\n",
    "def show_gau_light(ibl):\n",
    "    light_np = gaussian_filter(ibl, sigma=3)/np.max(ibl)\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.imshow(light_np)\n",
    "\n",
    "for i in range(6400):\n",
    "    ibl = compute_IBL(alpha_ary[i], beta_ary[i])\n",
    "    show_gau_light(ibl)\n",
    "\n",
    "    light_img = '{}/{}_light.png'.format('/home/ysheng/Dataset/soft_shadow/single_human/notsimulated_combine_male_short_outfits_genesis8_armani_casualoutfit03_Base_Pose_Standing_A',prefix_list[i])\n",
    "    # light_img_np = np.array(Image.open(light_img).resize((32,32)))\n",
    "    light_img_np = np.array(Image.open(light_img))\n",
    "    show_gau_light(light_img_np)\n",
    "    \n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
